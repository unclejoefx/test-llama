{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://flyairpeace.com/'\n",
    "path = '../data/flights'\n",
    "bs_parser = 'html.parser'\n",
    "delay_sec = 5\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(url):\n",
    "    \"\"\"Fetch the page and return a BeautifulSoup object.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            print(f\"Failed to fetch {url} with status code {response.status_code}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_links(html_content):\n",
    "    \"\"\"Extract HTML content.\"\"\"\n",
    "    links = []\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')  # Convert string to BeautifulSoup object here\n",
    "    articles = soup.find_all('div', class_='post-item style2 no-padding')\n",
    "    for article in articles:\n",
    "        link = article.find('a', href=True)\n",
    "        if link:\n",
    "            links.append(link['href'])\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Extract the title\n",
    "    title_section = soup.find('h1', class_='page-title')\n",
    "    data['title'] = title_section.get_text(strip=True) if title_section else 'No Title Found'\n",
    "\n",
    "    # Initialize the body content\n",
    "    body_content = []\n",
    "\n",
    "    # Process the main content section\n",
    "    main_content = soup.find('div', class_='post-desc')\n",
    "    if main_content:\n",
    "        # Collect text from all paragraphs until 'Schools' section\n",
    "        for element in main_content.next_elements:\n",
    "            if isinstance(element, Tag):\n",
    "                if element.name == 'h2' and \"Real Estate\" in element.text:\n",
    "                    break  # Stop processing if 'Real Estate' section is reached\n",
    "                if element.name == 'p':\n",
    "                    body_content.append(element.get_text(strip=True))\n",
    "\n",
    "    data['body'] = ' '.join(body_content)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_content_to_markdown(data, filename):\n",
    "    \"\"\"Convert extracted data to markdown and save to a file.\"\"\"\n",
    "    title_md = md(f\"# {data['title']}\")\n",
    "    body_md = md(data['body'])\n",
    "    markdown_content = f\"{title_md}\\n\\n{body_md}\"\n",
    "    with open(os.path.join(path, f\"{filename}.md\"), 'w') as file:\n",
    "        file.write(markdown_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article_links(links):\n",
    "    for link in links:\n",
    "        html_content = fetch_page(link)\n",
    "        if html_content:\n",
    "            content_data = extract_content(html_content)\n",
    "            save_content_to_markdown(content_data, link.split('/')[-1])\n",
    "            print(f\"Data from {link}:\")\n",
    "            print(content_data)\n",
    "        else:\n",
    "            print(f\"Failed to process {link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    base_url = 'https://nigeriapropertycentre.com/area-guides'\n",
    "    page_links = [f\"{base_url}?page={i}\" for i in range(1, 11)]  # Pages 1 to 10\n",
    "    all_links = []\n",
    "    \n",
    "    for page_link in page_links:\n",
    "        soup = fetch_page(page_link)\n",
    "        if soup:\n",
    "            article_links = get_article_links(soup)\n",
    "            all_links.extend(article_links)\n",
    "            print(f\"Found {len(article_links)} links on {page_link}\")\n",
    "\n",
    "    # Display all unique links gathered\n",
    "    unique_links = list(set(all_links))  # Remove duplicates if any\n",
    "    print(f\"Total unique links found: {len(unique_links)}\")\n",
    "    for link in unique_links:\n",
    "        print(link)\n",
    "\n",
    "    process_article_links(unique_links)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
